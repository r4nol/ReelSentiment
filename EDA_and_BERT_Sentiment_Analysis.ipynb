{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYjrWHZLPqLP"
      },
      "source": [
        "# Sentiment Analysis on IMDb Dataset\n",
        "This notebook demonstrates a simple end-to-end pipeline for sentiment analysis using BERT and Hugging Face Transformers.\n",
        "\n",
        "We will:\n",
        "1. Load the IMDb dataset\n",
        "2. Perform basic EDA (exploratory data analysis) with Seaborn\n",
        "3. Fine-tune a BERT model using the Trainer API\n",
        "4. Save the model and tokenizer\n",
        "5. Load the saved model for inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JamOP40sPqLQ"
      },
      "source": [
        "# Installing all dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIwL8sg1PqLR"
      },
      "outputs": [],
      "source": [
        "!pip install numpy pandas seaborn matplotlib torch transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXzjgz_3PqLR"
      },
      "source": [
        "# 1. Imports and basic setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKuTXsT-PqLR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from evaluate import load as load_metric\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Check if we have a GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBN3GFMNPqLS"
      },
      "source": [
        "# 2. Load the IMDb dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VgR2VsZPqLS"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "print(dataset)\n",
        "\n",
        "# Convert training split to a pandas DataFrame for EDA\n",
        "df_train = dataset[\"train\"].to_pandas()\n",
        "df_test = dataset[\"test\"].to_pandas()\n",
        "\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B36MdwPCPqLS"
      },
      "source": [
        "# 3. Basic EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Check class distribution"
      ],
      "metadata": {
        "id": "cOOtwlw9QXKP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnObov9YPqLT"
      },
      "outputs": [],
      "source": [
        "df_train['sentiment'] = df_train['label'].apply(lambda x: 'Positive' if x == 1 else 'Negative')\n",
        "\n",
        "sns.countplot(data=df_train, x='sentiment')\n",
        "plt.title(\"Sentiment Distribution in Training Set\")\n",
        "plt.show()\n",
        "\n",
        "positive_count = (df_train['label'] == 1).sum()\n",
        "negative_count = (df_train['label'] == 0).sum()\n",
        "print(f\"Positive reviews: {positive_count}, Negative reviews: {negative_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdkdaC7NPqLT"
      },
      "source": [
        "## 3.2. Analyze text length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spLX7jANPqLT"
      },
      "outputs": [],
      "source": [
        "df_train['text_length'] = df_train['text'].apply(lambda x: len(x.split()))\n",
        "print(df_train['text_length'].describe())\n",
        "\n",
        "sns.histplot(data=df_train, x='text_length', bins=50, hue='sentiment')\n",
        "plt.title(\"Distribution of Text Length\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kd3iqXPPqLT"
      },
      "source": [
        "# 4. Prepare data for fine-tuning BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Load BertTokenizer"
      ],
      "metadata": {
        "id": "E4qVxjFEQdLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEEHv7V1PqLT"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "encoded_dataset = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W-kud-dPqLU"
      },
      "source": [
        "## 4.2. Split into train/val/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK8N7FUoPqLU"
      },
      "outputs": [],
      "source": [
        "train_full = encoded_dataset['train']\n",
        "test_dataset = encoded_dataset['test']\n",
        "\n",
        "split_data = train_full.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split_data['train']\n",
        "val_dataset = split_data['test']\n",
        "\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Validation size:\", len(val_dataset))\n",
        "print(\"Test size:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO9hiyrNPqLU"
      },
      "source": [
        "## 4.3. Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b3OZEZjPqLU"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=2\n",
        ")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z4-yOKBPqLU"
      },
      "source": [
        "# 5. Fine-tuning with Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVbLBFE4PqLU"
      },
      "outputs": [],
      "source": [
        "accuracy_metric = load_metric(\"accuracy\")\n",
        "f1_metric = load_metric(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
        "    f1 = f1_metric.compute(predictions=preds, references=labels)[\"f1\"]\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCfD5gaXPqLU"
      },
      "source": [
        "# 6. Evaluation on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYgOD9-uPqLV"
      },
      "outputs": [],
      "source": [
        "test_metrics = trainer.evaluate(test_dataset)\n",
        "print(\"Evaluation on test set:\", test_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuY2FBpsPqLV"
      },
      "source": [
        "# 7. Save the model and tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS2VfyJFPqLV"
      },
      "outputs": [],
      "source": [
        "model_dir = \"output\"\n",
        "\n",
        "# Optionally set readable labels\n",
        "model.config.id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "model.config.label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "\n",
        "# Save model\n",
        "trainer.save_model(model_dir)  # saves model weights and config\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXn3NBK5PqLV"
      },
      "source": [
        "# 8. Inference with the saved model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEPchJsRPqLV"
      },
      "outputs": [],
      "source": [
        "loaded_tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
        "loaded_model = BertForSequenceClassification.from_pretrained(model_dir)\n",
        "loaded_model.to(device)\n",
        "loaded_model.eval()\n",
        "\n",
        "def predict_sentiment(text_list):\n",
        "    if isinstance(text_list, str):\n",
        "        text_list = [text_list]\n",
        "    inputs = loaded_tokenizer(\n",
        "        text_list,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = loaded_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    results = []\n",
        "    for i, pred in enumerate(preds):\n",
        "        label_id = pred.item()\n",
        "        label_str = loaded_model.config.id2label[label_id]\n",
        "        results.append({\n",
        "            'text': text_list[i],\n",
        "            'label': label_str\n",
        "        })\n",
        "    return results\n",
        "\n",
        "sample_texts = [\n",
        "    \"I absolutely loved this movie. It's fantastic!\",\n",
        "    \"Worst film ever. Completely boring and pointless.\"\n",
        "]\n",
        "\n",
        "predictions = predict_sentiment(sample_texts)\n",
        "for p in predictions:\n",
        "    print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1EWEoamPqLV"
      },
      "source": [
        "## 9. Conclusions\n",
        "- The IMDb dataset is balanced, containing roughly an equal number of positive and negative reviews.\n",
        "- Simple EDA shows that many reviews are relatively long.\n",
        "- Fine-tuning BERT on 2 epochs can already yield high accuracy.\n",
        "- Setting `id2label` and `label2id` allows us to get human-readable labels (POSITIVE/NEGATIVE) instead of LABEL_0/LABEL_1.\n",
        "- We can now deploy this model by loading it in any environment that supports PyTorch and Hugging Face Transformers.\n",
        "\n",
        "Thank you for checking this notebook!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}